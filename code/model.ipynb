{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import statsmodels.api as sm\n",
    "\n",
    "model_data_df = pd.read_csv('../data/boston.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_features_df = model_data_df[['CRIM','RM','LSTAT']]\n",
    "dependent_feature_df = model_data_df['MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   MEDV   R-squared:                       0.646\n",
      "Model:                            OLS   Adj. R-squared:                  0.644\n",
      "Method:                 Least Squares   F-statistic:                     305.2\n",
      "Date:                Sat, 28 Oct 2023   Prob (F-statistic):          1.01e-112\n",
      "Time:                        15:50:38   Log-Likelihood:                -1577.6\n",
      "No. Observations:                 506   AIC:                             3163.\n",
      "Df Residuals:                     502   BIC:                             3180.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -2.5623      3.166     -0.809      0.419      -8.783       3.658\n",
      "CRIM          -0.1029      0.032     -3.215      0.001      -0.166      -0.040\n",
      "RM             5.2170      0.442     11.802      0.000       4.348       6.085\n",
      "LSTAT         -0.5785      0.048    -12.135      0.000      -0.672      -0.485\n",
      "==============================================================================\n",
      "Omnibus:                      171.754   Durbin-Watson:                   0.822\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              628.308\n",
      "Skew:                           1.535   Prob(JB):                    3.67e-137\n",
      "Kurtosis:                       7.514   Cond. No.                         216.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Add a constant term to the independent variable matrix for the intercept\n",
    "X_with_intercept = sm.add_constant(independent_features_df)\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = sm.OLS(dependent_feature_df, X_with_intercept)\n",
    "results = model.fit()\n",
    "\n",
    "# Display the summary table\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary table produced by the linear regression model in the context of the Boston Housing dataset provides a comprehensive overview of the model's performance and the relationships between the independent and dependent variables. Let's break down the key components of the summary table in a more detailed manner:\n",
    "### 1. Dependent Variable (MEDV):\n",
    "\n",
    "Meaning: This is the variable we are trying to predictâ€” the median value of owner-occupied homes in $1000s.\n",
    "\n",
    "### 2. Model:\n",
    "\n",
    "Meaning: OLS (Ordinary Least Squares) is the method used for estimating the coefficients of the linear regression model.\n",
    "\n",
    "### 3. Coefficients:\n",
    "\n",
    "Intercept (const): Represents the estimated value of the dependent variable when all independent variables are zero.\n",
    "\n",
    "RM, CRIM, LSTAT: Represents the change in the dependent variable for a one-unit change in the independent variable respectively.\n",
    "    \n",
    "### 4. Coefficient Statistics:\n",
    "\n",
    "coef (Coefficient Estimate): The estimated value of the coefficient.\n",
    "\n",
    "std err (Standard Error): Indicates the precision of the estimate. Smaller values are preferable.\n",
    "\n",
    "t (t-statistic): The t-statistic tests the null hypothesis that the coefficient is equal to zero. Larger absolute values indicate more significance.\n",
    "\n",
    "P>|t| (p-value): The p-value associated with the t-statistic. A small p-value (< 0.05) suggests that the variable is statistically significant.\n",
    "\n",
    "[0.025 0.975] (Confidence Interval): The 95% confidence interval for the coefficient. If it contains zero, the variable may not be significant.\n",
    "\n",
    "### 5. Model Fit:\n",
    "\n",
    "R-squared: Represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). A higher R-squared indicates a better fit.\n",
    "\n",
    "Adj. R-squared: Similar to R-squared but adjusted for the number of predictors. It penalizes excessive use of variables.\n",
    "\n",
    "### 6. F-statistic:\n",
    "\n",
    "Meaning: A measure of how well the overall linear regression model fits the data. Tests the null hypothesis that all coefficients are equal to zero.\n",
    "\n",
    "Prob (F-statistic): The p-value associated with the F-statistic. A small p-value indicates that at least one independent variable is significant.\n",
    "\n",
    "### 7. Degrees of Freedom:\n",
    "\n",
    "Df Residuals: Degrees of freedom of the residuals. It's the difference between the number of observations and the number of parameters estimated.\n",
    "\n",
    "Df Model: Degrees of freedom of the model. It's the number of estimated parameters excluding the constant term.\n",
    "\n",
    "### 8. Covariance Type:\n",
    "Meaning: Here \"nonrobust\" refers to the type of covariance estimation used in calculating the standard errors of the coefficients. \n",
    "Let's break down what \"nonrobust\" means in this context:\n",
    "\n",
    "Covariance Type:\n",
    "\n",
    "    Meaning: Covariance represents the measure of how much two variables change together. In linear regression, the standard errors of the coefficients are calculated based on the estimated covariance matrix.\n",
    "\n",
    "    Types:\n",
    "\n",
    "        Robust Covariance: It accounts for potential violations of assumptions such as heteroscedasticity and outliers. Robust covariance estimators, like Huber-White or White's covariance, are less sensitive to the presence of outliers.\n",
    "\n",
    "        Nonrobust Covariance: It assumes that the residuals are homoscedastic (constant variance) and normally distributed. It is more sensitive to outliers.\n",
    "\n",
    "Implications of \"nonrobust\":\n",
    "\n",
    "    Assumption: The use of \"nonrobust\" implies that the standard errors of the coefficients are calculated under the assumption of homoscedasticity (constant variance) and normal distribution of residuals.\n",
    "\n",
    "    Sensitivity: Nonrobust estimates may be more sensitive to outliers or violations of assumptions in the data.\n",
    "\n",
    "    Considerations: Researchers might choose nonrobust covariance when they have confidence in the assumptions of homoscedasticity and normality. However, in the presence of potential violations, a robust covariance type might be preferred for more reliable standard error estimates.\n",
    "\n",
    "Practical Considerations:\n",
    "\n",
    "    Robust vs. Nonrobust: The choice between robust and nonrobust covariance depends on the characteristics of the data and the researcher's preference. If there are concerns about outliers or heteroscedasticity, robust covariance might be more appropriate.\n",
    "\n",
    "    Diagnostic Tools: Diagnostic tests for residuals, such as residual plots and tests for homoscedasticity, can guide the choice between robust and nonrobust covariance types.\n",
    "\n",
    "    Reporting: In research papers or reports, it's essential to specify the covariance type used in the analysis to ensure transparency and reproducibility.\n",
    "\n",
    "In summary, \"Covariance Type: nonrobust\" indicates that the standard errors of the coefficients in the linear regression model are calculated under the assumption of homoscedasticity and normality, making them more sensitive to potential violations of these assumptions.\n",
    "\n",
    "### 9. Omnibus Test:\n",
    "\n",
    "Meaning: The Omnibus test is a statistical test that assesses the overall goodness of fit of the model. In the context of linear regression, it tests the null hypothesis that the model as a whole does not explain a significant amount of variance in the dependent variable.\n",
    "\n",
    "Interpretation:\n",
    "    A small p-value (typically < 0.05) suggests that the model as a whole is statistically significant.\n",
    "    It provides a general indication of whether the set of independent variables has explanatory power.\n",
    "\n",
    "### 10. Skewness:\n",
    "\n",
    "Meaning: Skewness measures the asymmetry of the distribution of residuals. It indicates whether the residuals are symmetrically distributed around the mean.\n",
    "\n",
    "Interpretation:\n",
    "    Skewness close to zero suggests a roughly symmetric distribution of residuals.\n",
    "    Positive skewness indicates that the distribution has a longer right tail.\n",
    "    Negative skewness indicates that the distribution has a longer left tail.\n",
    "\n",
    "### 11. Kurtosis:\n",
    "\n",
    "Meaning: Kurtosis measures the \"tailedness\" of the distribution of residuals. It indicates the heaviness of the tails relative to a normal distribution.\n",
    "\n",
    "Interpretation:\n",
    "    Kurtosis close to zero suggests a distribution similar to the normal distribution.\n",
    "    Positive kurtosis indicates heavier tails (more extreme values) than the normal distribution.\n",
    "    Negative kurtosis indicates lighter tails than the normal distribution.\n",
    "\n",
    "### Why Are Skewness and Kurtosis Important?\n",
    "\n",
    "    Assumption Checking: Skewness and kurtosis are essential for checking the assumptions of a linear regression model. Normality of residuals is a common assumption, and these statistics provide insights into the distribution of residuals.\n",
    "\n",
    "    Model Performance: The Omnibus test, along with skewness and kurtosis, helps evaluate the overall performance of the model. Significant results in the Omnibus test may prompt further investigation into the model's fit.\n",
    "\n",
    "    Residual Analysis: Skewness and kurtosis of residuals give an indication of how well the residuals align with the assumptions of normality.\n",
    "\n",
    "\n",
    "### 12. Durbin-Watson Statistic:\n",
    "\n",
    "Meaning: The Durbin-Watson statistic is a test for autocorrelation in the residuals. It assesses whether there is a pattern or correlation between successive residuals.\n",
    "\n",
    "Interpretation:\n",
    "    The Durbin-Watson statistic ranges from 0 to 4.\n",
    "    A value around 2 suggests no autocorrelation.\n",
    "    Values significantly below 2 suggest positive autocorrelation (residuals are correlated positively with adjacent residuals).\n",
    "    Values significantly above 2 suggest negative autocorrelation (residuals are correlated negatively with adjacent residuals).\n",
    "\n",
    "Importance: Autocorrelation in residuals violates the assumption of independence, which is crucial for the validity of regression analysis. Durbin-Watson helps detect this issue.\n",
    "\n",
    "### 13. Jarque-Bera Test:\n",
    "\n",
    "Meaning: The Jarque-Bera test is a goodness-of-fit test that assesses whether the residuals of a regression model have the skewness and kurtosis matching a normal distribution.\n",
    "\n",
    "Interpretation:\n",
    "    A small p-value (typically < 0.05) indicates that the residuals do not follow a normal distribution.\n",
    "    High values of the test statistic suggest non-normality.\n",
    "\n",
    "Importance: The normality assumption is vital for the reliability of statistical inferences based on the regression model. Deviation from normality might affect the accuracy of hypothesis tests and confidence intervals.\n",
    "\n",
    "### Why Are Durbin-Watson Statistic and Jarque-Bera Test Important?\n",
    "\n",
    "Durbin-Watson: Helps identify and address autocorrelation issues in the residuals, ensuring the validity of statistical inferences.\n",
    "\n",
    "Jarque-Bera Test: Assesses the normality assumption of residuals. Deviations from normality may indicate that the model is not capturing certain patterns in the data.\n",
    "\n",
    " \n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "This comprehensive summary table allows us to assess the significance of each variable, understand the overall fit of the model, and identify potential areas for improvement. It's a powerful tool for interpreting the results of a linear regression analysis on the Boston Housing dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
